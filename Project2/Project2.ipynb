{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Deep Learning\n",
    "\n",
    "## Part 1: Theory Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Alternative approach - Backpropagtion\n",
    "\n",
    "Instead of using the proposed approach, that finds expressions for updating $W$ and the $K_m$ directly, we could also use the backpropagation principle, that is used in the Deep Learning community successfully.\n",
    "The idea is to make use of the chain rule and computing the gradients for intermediate stages (\"divide and conquer\"). \n",
    "First of all we find the computational graph for our problem (see Figure).\n",
    "<img src=\"files/compGraph.PNG\">\n",
    "\n",
    "In the graph we start from $Y_0$ and \"unroll\" the Euler steps, that are computed one after each other before in the end the objective function $J$ is computed.\n",
    "As in the other approach we seek for the expressions $\\frac{\\partial J}{\\partial W}$ and $\\frac{\\partial J}{\\partial K_m}$.\n",
    "To find an expression for the gradient of $J$ w.r.t. $W$, we follow the graph from $J$ to $W$ and find this as:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial W} =\n",
    "    \\frac{\\partial J}{\\partial A} \\cdot\n",
    "    \\frac{\\partial A}{\\partial B} \\cdot\n",
    "    \\frac{\\partial B}{\\partial D} \\cdot\n",
    "    \\frac{\\partial D}{\\partial W}\n",
    "$$\n",
    "\n",
    "Note that $A, B, D \\in \\mathbb{R}^{N}$ \n",
    "Now we have to find the single intermediate stage derivatives.\n",
    "\n",
    "$$\n",
    "    J = \\frac{1}{2} || A ||^2_2 = \\frac{1}{2} A^TA\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial A} = A\n",
    "$$\n",
    "\n",
    "$$\n",
    "    A = B-C\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial A}{\\partial B} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "    B = \\eta(D)\n",
    "$$\n",
    "\n",
    "$\\eta$ is the in binary classification often used sigmoid function. It provides a valid probability distribution over the classes 0 and 1.\n",
    "The derivative of the sigmoid function is know as:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial B}{\\partial D} = \\eta(D) \\cdot (1-\\eta(D))\n",
    "$$\n",
    "\n",
    "$$\n",
    "    D = Y_M \\cdot W\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial D}{\\partial W} = Y_M\n",
    "$$\n",
    "\n",
    "Putting all together we find\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial W} =\n",
    "    \\frac{\\partial J}{\\partial A} \\cdot\n",
    "    \\frac{\\partial A}{\\partial B} \\cdot\n",
    "    \\frac{\\partial B}{\\partial D} \\cdot\n",
    "    \\frac{\\partial D}{\\partial W}\n",
    "    =\n",
    "    Y_M^T \\left( \\eta(B-C) \\odot \\eta(D) \\odot (1-\\eta(D)) \\right)\n",
    "$$\n",
    "\n",
    "This is nothing else but the solution given, but in a backpropagation formulation.\n",
    "\n",
    "The same procedure is done to find the $\\frac{\\partial J}{\\partial Y_M}$. As we go down the same way on the computational graph as before only need a result for the last step.\n",
    "\n",
    "$$\n",
    "    D = Y_M \\cdot W\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial D}{\\partial Y_M} = W\n",
    "$$\n",
    "\n",
    "Putting all together we find\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial Y_M} =\n",
    "    \\frac{\\partial J}{\\partial A} \\cdot\n",
    "    \\frac{\\partial A}{\\partial B} \\cdot\n",
    "    \\frac{\\partial B}{\\partial D} \\cdot\n",
    "    \\frac{\\partial D}{\\partial Y_M}\n",
    "    =\n",
    "    \\left( \\eta(B-C) \\odot \\eta(D) \\odot (1-\\eta(D)) \\right)W^T\n",
    "$$\n",
    "\n",
    "The derivative $\\frac{\\partial J}{\\partial Y_M}$ can now be used as upstream gradient while going through the last Euler step. Finding the derivative $\\frac{\\partial Y_{m+1}}{\\partial Y_{m}}$ we can then pass the gradient from one Euler step to the previous. The derivatives $\\frac{\\partial J}{\\partial Y_{m}}$ and $\\frac{\\partial J}{\\partial K_{m}}$, we are actually interested in, can be computed as\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial Y_{m}} =\n",
    "    \\frac{\\partial J}{\\partial Y_{m+1}} \\cdot \\frac{\\partial Y_{m+1}}{\\partial Y_{m}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial K_{m}} =\n",
    "    \\frac{\\partial J}{\\partial Y_{m+1}} \\cdot \\frac{\\partial Y_{m+1}}{\\partial K_{m}}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial J}{\\partial Y_{m+1}}$ is the upstream gradient we get from the upper Euler step, that we already computed. This is what makes backpropagation so powerful here.\n",
    "\n",
    "So within one Euler step we compute the gradient to pass further:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Y_{m+1}}{\\partial Y_{m}} =\n",
    "    \\frac{Y_{m+1}}{\\partial a} \\cdot\n",
    "    \\frac{\\partial a}{\\partial b} \\cdot\n",
    "    \\frac{\\partial b}{\\partial c} \\cdot\n",
    "    \\frac{\\partial c}{\\partial Y_{m}}\n",
    "    + \\frac{\\partial Y_{m+1}}{\\partial d}\n",
    "$$\n",
    "\n",
    "The single expressions are:\n",
    "\n",
    "$$\n",
    "    Y_{m+1} = a + d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Y_{m+1}}{\\partial a} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Y_{m+1}}{\\partial d} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "   a = h \\cdot b\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial a}{\\partial b} = h\n",
    "$$\n",
    "\n",
    "$$\n",
    "   b = \\sigma(c) = \\tanh(c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial b}{\\partial c} = 1 - \\tanh(c)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "    c = Y_m \\cdot K_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial c}{\\partial Y_m} = K_m\n",
    "$$\n",
    "\n",
    "This leads to:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Y_{m+1}}{\\partial Y_{m}} =\n",
    "    \\frac{Y_{m+1}}{\\partial a} \\cdot\n",
    "    \\frac{\\partial a}{\\partial b} \\cdot\n",
    "    \\frac{\\partial b}{\\partial c} \\cdot\n",
    "    \\frac{\\partial c}{\\partial Y_{m}}\n",
    "    + \\frac{\\partial Y_{m+1}}{\\partial d}\n",
    "    =\n",
    "    \\left(h \\cdot \\left( 1 - \\tanh(c)^2 \\right) \\right) K_m^T + 1\n",
    "$$\n",
    "\n",
    "For the gradient for $K_m$ we go down the graph the same way but the last stage.\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial c}{\\partial K_m} = Y_m\n",
    "$$\n",
    "\n",
    "We find:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Y_{m+1}}{\\partial K_{m}} =\n",
    "    \\frac{Y_{m+1}}{\\partial a} \\cdot\n",
    "    \\frac{\\partial a}{\\partial b} \\cdot\n",
    "    \\frac{\\partial b}{\\partial c} \\cdot\n",
    "    \\frac{\\partial c}{\\partial K_{m}}\n",
    "    =\n",
    "    Y_m^T \\left(h \\cdot \\left( 1 - \\tanh(c)^2 \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementation\n",
    "\n",
    "Our implementation is split into 2 parts. One is the brute force approach with numerical gradient computation as specified in the problem sheet. The other is the variant using analytical gradient computation and some additional features, known from Deep Learning, that will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nl\n",
    "import matplotlib.pyplot as plt\n",
    "import make_circle_problem as mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler (forward Model)\n",
    "The Euler function for the forward model as specified in the problem sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euler(M, h, K, Y0, sigma):\n",
    "    Y = np.array(Y0)\n",
    "    for i in range(M):\n",
    "        Y += h * sigma(np.dot(Y, K[i]))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save recomputation of intermediate results in the backpropagation, we store intermediate results in this variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EulerBackProp(M, h, K, Y0, sigma):\n",
    "    Y_list = []\n",
    "    sigma_list = []\n",
    "    Y = np.array(Y0)\n",
    "    Y_list.append(np.copy(Y))\n",
    "    for i in range(M):\n",
    "        sigma_value = sigma(np.dot(Y, K[i]))\n",
    "        Y += h * sigma_value\n",
    "        sigma_list.append(np.copy(sigma_value))\n",
    "        Y_list.append(np.copy(Y))\n",
    "    return Y, sigma_list, Y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function calculates an error estimate using the current K and W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObjFunc(M, h, K, Y0, sigma, eta, C, W):\n",
    "    YM = Euler(M, h, K, Y0, sigma)\n",
    "    projv = eta(np.dot(YM, W))\n",
    "    return (1/2)*nl.norm(projv - C)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etader(x):\n",
    "    return np.exp(x)/(np.exp(x)+1)**2\n",
    "\n",
    "def sigmader(x):\n",
    "    return 1/np.cosh(x)**2\n",
    "\n",
    "\n",
    "def Wgrad(M, h, K, Y0, sigma, eta, C, W):\n",
    "    YM = Euler(M, h, K, Y0, sigma)\n",
    "    y = np.dot(YM, W)\n",
    "    return np.dot(YM.T, etader(y)*(eta(y) - C))\n",
    "\n",
    "\n",
    "def Kgrad(M, h, K, Y0, sigma, eta, C, W):\n",
    "    Y, sigma_list, Y_list = EulerBackProp(M, h, K, Y0, sigma)\n",
    "    y = np.dot(Y, W)\n",
    "    WT = np.reshape(W, (1,4))\n",
    "    dJdY = etader(y)*(eta(y)-C)\n",
    "    dJdY = np.reshape(dJdY, (np.size(dJdY), 1))\n",
    "    dJdY = np.dot(dJdY, WT)\n",
    "    dK = np.zeros((M,4,4))\n",
    "    U = np.array(dJdY)\n",
    "    for i in range(M):\n",
    "        U += np.dot(h*(sigmader(np.dot(Y_list[M-(i+1)],K[M-1-i]))*U), K.T)[...,:,0]\n",
    "        arg = h*(sigmader(np.dot(Y_list[M-i-1], K[M-1-i])))*U\n",
    "        dK[M-i-1] = np.dot(Y_list[M-i-1].T, arg)\n",
    "    return dK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Gradient Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function GradCalc computes the gradient of J numerically using the procedure given in the project description. It introduces a small pertubation to one of the Ks or W and calculates the derivative of J with respect to K or W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradCalc(M, h, K, Y0, sigma, eta, C, W, eps):\n",
    "    dJ = np.zeros((M, 4, 4))\n",
    "    j1 = ObjFunc(M, h, K, Y0, sigma, eta, C, W)\n",
    "    for m in range(M):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                temp = np.zeros((4,4))\n",
    "                temp[i,j] = eps\n",
    "                dK = np.array(K)\n",
    "                dK[m] += temp\n",
    "                j2 = ObjFunc(M, h, dK, Y0, sigma, eta, C, W)\n",
    "                dJ[m,i,j] = (j2-j1)/eps\n",
    "\n",
    "    dW = np.zeros(4)\n",
    "    w1 = ObjFunc(M, h, K, Y0, sigma, eta, C, W)\n",
    "    for i in range(4):\n",
    "        temp = np.zeros(4)\n",
    "        temp[i] = eps\n",
    "        Wp = np.array(W) + temp\n",
    "        w2 = ObjFunc(M, h, K, Y0, sigma, eta, C, Wp)\n",
    "        dW[i] = (w2-w1)/eps\n",
    "    return dJ, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation variant\n",
    "\n",
    "For the backpropagation variant, we put forward and backward model in one class, to use intermediate results of the forward computation in the backward computation.\n",
    "One can see the implementation of formulas derived before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObjFuncAndBackProp(M, h, K, Y0, sigma, eta, C, W):\n",
    "    #Forward model\n",
    "    YM, sigma_list, Y_list = EulerBackProp(M, h, K, Y0, sigma)\n",
    "    projv = eta(np.dot(YM, W))\n",
    "    error = (1/2)*nl.norm(projv - C)**2\n",
    "\n",
    "    #Backpropagation\n",
    "    #deriv. w.r.t. W\n",
    "    dW = np.dot(YM.T, ((projv - C) * projv * (1 - projv)))    # * is elementwise (hadamard) operation\n",
    "\n",
    "    #deriv. w.r.t. YM\n",
    "    dYM = np.array(np.matrix((projv - C) * projv * (1 - projv)).T * np.matrix(W.T))  # * is elementwise (hadamard) operation\n",
    "\n",
    "    #deriv. w.r.t. the different K_m\n",
    "    #Starting with dYM as first upstream derivative we pass the gradient through the computational graph\n",
    "    dJ = np.zeros((M, 4, 4))\n",
    "    dY_upstream = dYM\n",
    "    for i in range(len(Y_list)-2, -1, -1):\n",
    "        dJ[i] = Y_list[i].T.dot(h * (1 - sigma_list[i] ** 2) * dY_upstream)\n",
    "        dY_upstream = np.dot((h * (1 - sigma_list[i] ** 2) * dY_upstream), K[i].T) + dY_upstream\n",
    "    return error, dJ, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter update\n",
    "\n",
    "For the parameter update we have to different options. First is the proposed standard gradient descent (GD) update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_update(x, dx, tau):\n",
    "    return x - tau * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second option is the ADAM method. This method is currently the most used update method in the Deep Learning community, due to its beneficial effects.\n",
    "While the GD uses just the gradient itself for updating the parameters, ADAM introduces a so called \"momentum\". The meaning of this can be explained in a phyical analogy of a ball with friction.\n",
    "While the ball rolls in one direction for a longer time, it builds up velocity. Even if there are small valleys in the topology, due to the velocity it can (probably) roll out of those again and eventually roll to the lowest point.\n",
    "In the context of nonlinear optimization this means we (probably) can avoid getting stuck in local minima as we seek for the global minimum and as well get faster to the minimum as we use the momentum.\n",
    "The deviation and updates formulas can be found in the original paper (https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(value, g_t, t, m_t_old, v_t_old):\n",
    "    t += 1\n",
    "    m_t = beta_1 * m_t_old + (1 - beta_1) * g_t  # updates the moving averages of the gradient\n",
    "    v_t = beta_2 * v_t_old + (1 - beta_2) * (g_t * g_t)  # updates the moving averages of the squared gradient\n",
    "    m_cap = m_t / (1 - (beta_1 ** t))  # calculates the bias-corrected estimates\n",
    "    v_cap = v_t / (1 - (beta_2 ** t))  # calculates the bias-corrected estimates\n",
    "    value -= np.divide((alpha * m_cap), (np.sqrt(v_cap) + epsilon))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Our training method computes forward and backward model and updates the parameters. The user can choose which variant to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(M, h, K, Y0, sigma, eta, C, W, eps, TOL, tau, gradient_variant=1, update_variant=1, max_step=100000):\n",
    "    res = np.inf\n",
    "    res_list = []\n",
    "    count = 0\n",
    "    \n",
    "    #Parameter Adam\n",
    "    m_t_K = 0\n",
    "    v_t_K = 0\n",
    "    m_t_W = 0\n",
    "    v_t_W = 0\n",
    "    \n",
    "    while res > TOL and count < max_step:\n",
    "        #numerical\n",
    "        if gradient_variant == 0:\n",
    "            res = ObjFunc(M, h, K, Y0, sigma, eta, C, W)\n",
    "            dJ, dW = GradCalc(M, h, K, Y0, sigma, eta, C, W, eps)\n",
    "        #analytically - backpropagation\n",
    "        if gradient_variant == 1:\n",
    "            res, dJ, dW = ObjFuncAndBackProp(M, h, K, Y0, sigma, eta, C, W)\n",
    "        \n",
    "        #analytically\n",
    "        if gradient_variant == 2:\n",
    "            res = ObjFunc(M, h, K, Y0, sigma, eta, C, W)\n",
    "            dW = Wgrad(M, h, K, Y0, sigma, eta, C, W)\n",
    "            dJ = Kgrad(M, h, K, Y0, sigma, eta, C, W)\n",
    "        \n",
    "        #Updates\n",
    "        #Stochastic gradient descent (as proposed in the problem sheet)\n",
    "        if update_variant == 0:\n",
    "            K = SGD_update(K, dJ, tau)\n",
    "            W = SGD_update(W, dW, tau)\n",
    "\n",
    "        #Adam updates\n",
    "        if update_variant == 1:\n",
    "            W = adam_update(W, dW, count, m_t_W, v_t_W)\n",
    "            K = adam_update(K, dJ, count, m_t_K, v_t_K)\n",
    "\n",
    "        if count % 5000 == 0:\n",
    "            print(\"Residual at step \" + str(count) + \": \" + str(res))\n",
    "        res_list.append(res)\n",
    "        count += 1\n",
    "    return K, W, res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions eta and sigma are implemented as given in the project description. The function res_plot plots the residual versus steps, and get_accuracy checks how well the model predicts the color of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta(x):\n",
    "    return np.exp(x)/(np.exp(x) + 1)\n",
    "\n",
    "def sigma(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def res_plot(res_list):\n",
    "    plt.plot(list(range(len(res_list))), res_list)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"residual\")\n",
    "    plt.title(\"Residual plot\")\n",
    "    plt.show()\n",
    "\n",
    "def get_accuracy(YM, W):\n",
    "    projv = eta(np.dot(YM, W))\n",
    "    guess = np.around(projv)\n",
    "    diff = guess - C\n",
    "    wrong_guesses = np.count_nonzero(diff)\n",
    "    accuracy = (1 - wrong_guesses / n)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the parameters are initialized, the model trained and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual at step 0: 63.4202715072\n",
      "Residual at step 5000: 38.3538617476\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-04393b67a3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m#res_plot(res_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9bd88f2dc0a0>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(M, h, K, Y0, sigma, eta, C, W, eps, TOL, tau, gradient_variant, update_variant, max_step)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e9911d9969e2>\u001b[0m in \u001b[0;36mKgrad\u001b[0;34m(M, h, K, Y0, sigma, eta, C, W)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdJdY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Number of points\n",
    "n = 500\n",
    "#Precision of points\n",
    "nx = 200\n",
    "#number of Euler steps\n",
    "M = 20\n",
    "eps = 0.005\n",
    "#Learning rate of GD\n",
    "tau = 0.1\n",
    "\n",
    "h = 0.1\n",
    "#tolerance depending on n\n",
    "TOL = 0.01 * n\n",
    "\n",
    "\n",
    "\n",
    "#Parameter ADAM\n",
    "alpha = 0.00005 #learning rate ADAM\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "#self-made function, to get data into useful shape\n",
    "Y0, C = mcp.YC(n, nx, False)\n",
    "K = np.full((M,4,4), np.identity(4), dtype=float)\n",
    "W = np.ones(4)\n",
    "for i in range(n):\n",
    "    Y0[i,2] = Y0[i,0]**2\n",
    "    Y0[i,3] = Y0[i,1]**2\n",
    "#arguments\n",
    "Eargs = (M, h, K, Y0, sigma)\n",
    "OFargs = (M, h, K, Y0, sigma, eta, C, W)\n",
    "GCargs = (M, h, K, Y0, sigma, eta, C, W, eps)\n",
    "Targs = (M, h, K, Y0, sigma, eta, C, W, eps, TOL, tau)\n",
    "\n",
    "#Training\n",
    "K, W, res_list = Train(*Targs)\n",
    "res_plot(res_list)\n",
    "\n",
    "#Accuracy check\n",
    "#Training data accuracy\n",
    "YM = Euler(M, h, K, Y0, sigma)\n",
    "accu = get_accuracy(YM, W)\n",
    "print(\"Accuracy on training set: \" + str(accu * 100) + \"%\")\n",
    "\n",
    "#Test data accuracy\n",
    "Y0, C = mcp.YC(n, nx, False)\n",
    "YM = Euler(M, h, K, Y0, sigma)\n",
    "accu = get_accuracy(YM, W)\n",
    "print(\"Accuracy on test set: \" + str(accu * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Comments\n",
    "\n",
    "a) As this process is a stochastic process, depending on which points are generated, it is not sure, that the the algorithm converges nicely to 100% training accuracy. Sometimes the model gets stuck in local minima. In this case you have to start the programm again.\n",
    "\n",
    "b) We see much faster gradient computation with the analytical version.\n",
    "\n",
    "c) We see much faster convergence with ADAM than with GD.\n",
    "\n",
    "d) If the model converges and we used few data points we see very good classification results for the training data (90-100%), but not for the test data. This is due to \"overfitting\". The model learns the few data points by hard and has poor generalization characteristics. To overcome this one would need to introduce regularization or use more data.\n",
    "\n",
    "e) As we one apply transformations from $\\mathbb{R}^4$ to $\\mathbb{R}^4$, we operate in the 4-dimensional-space only and therefore have not much \"capacity\" in the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
